---
title: "GLM"
author: "Dr. Mine Dogucu & Dr. Mozhdeh Forghani"
execute:
  echo: true
format: 
  revealjs:
    slide-number: true
    # logo: "https://socaldatascience.github.io/bootcamp-materials-2022/img/socalds-logo.png"
    # theme: ["slide-style.scss"]
    # incremental: false
---

## Packages

```{r}
#| message: false
# install.packages(c("MASS", "lattice", "sandwich", "boot", "COUNT", "pscl",
#                      "logistf"), packagesdependencies=TRUE)
library(MASS)
library(lattice)
library(sandwich)
library(boot)
library(COUNT)
library(pscl)
library(logistf)
library(ggplot2)

```

## Recall: Linear Regression Model
- Simple regression model

- Linear Regression model

- Assumption in OLS:

1 - Errors have mean equal to zero.

2 - Errors are independent.

3 - Errors have equal or constant variance (Homoscedasticity or Homogeneity of variance).

4 - Errors follow a normal distribution.


## Data

```{r}
dat <- read.csv(
  "https://stats.oarc.ucla.edu/wp-content/uploads/2019/02/elemapi2v2.csv")
head(dat[,c("api00", "enroll")])

```
##

```{r}
# We want to study the relationship between academic performance api00
# and number of students enrolled in schools,
#scatter plot
plot(api00 ~ enroll, data = dat,
     main = "Scatter plot of school performance vs. number of enrollment",
     ylab = "api00 (performance score in 2000)",
     xlab = "enroll", xlim = c(0,1000))

```

## Example of linear regression model using lm()

```{r}
# We want to study the relationship between academic performance api00
# and number of students enrolled in schools,

m1 <- lm(api00 ~ enroll, data = dat)
summary(m1)

opar <- par(mfrow = c(2, 2), mar = c(4.1, 4.1, 2.1, 1.1))
plot(m1)

```
## Interpretation



## 	Logistic Regression

There are cases that the outcome for a given set of predictors can only gets positive values or values in the interval (0,1)

-If the outcome is dichotomous and can get only value 0 and 1 (Bernoulli distribution), the mean is between 0 and 1.

-If the outcome is from a count distribution where it is count number 0,1,2,⋯ like Poisson distribution, the mean is always positive (𝜇>0).


## Link Function


  mean of Y :        0 < 𝜇 < 1
  
 . . .                     
    
          
  odds function:   0 < 𝜇/(1-𝜇) < ∞
  
  
 . . .
 
 
  logit function:  -∞ < log(𝜇/(1-𝜇)) < ∞

## Logistic regression model 

likelihood: $Y_i | \pi_i \sim \text{Bern}(\pi_i)$

. . .

$𝜇_i = E(Y_i|\pi_i) = \pi_i$

. . .

link function: $g(\pi_i)$

. . .

  $g(\pi_i)= \beta_0 + \beta_1 X_{i1}$

. . .

logit link function: $Y_i|\beta_0,\beta_1 \stackrel{ind}{\sim} \text{Bern}(\pi_i) \;\; \text{ with } \;\; \log\left(\frac{\pi_i}{1 - \pi_i}\right) = \beta_0 + \beta_1 X_{i1}$

. . .

$$\frac{\pi_i}{1-\pi_i} = e^{\beta_0 + \beta_1 X_{i1}}
\;\;\;\; \text{ and } \;\;\;\;
\pi_i = \frac{e^{\beta_0 + \beta_1 X_{i1}}}{1 + e^{\beta_0 + \beta_1 X_{i1}}}$$

##
  

$$\log(\text{odds}) = \log\left(\frac{\pi}{1-\pi}\right) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p \; . $$
When $(X_1,X_2,\ldots,X_p)$ are all 0, $\beta_0$ is the __typical log odds__ of the event of interest and $e^{\beta_0}$ is the __typical odds__.

When controlling for the other predictors $(X_2,\ldots,X_p)$, let $\text{odds}_x$ represent the typical odds of the event of interest when $X_1 = x$ and $\text{odds}_{x+1}$ the typical odds when $X_1 = x + 1$. Then when $X_1$ increases by 1, from $x$ to $x + 1$, $\beta_1$ is the typical __change in log odds__ and $e^{\beta_1}$ is the typical __multiplicative change in odds__:
    
    
##

$$\beta_1 = \log(\text{odds}_{x+1}) - \log(\text{odds}_x)
\;\;\; \text{ and } \;\;\; e^{\beta_1} = \frac{\text{odds}_{x+1}}{\text{odds}_x}$$

. . .

$$\log\left(\frac{\pi}{1-\pi}\right) = \beta_0 + \beta_1 X_1$$

. . .


$$\pi = \frac{e^{\beta_0 + \beta_1 X_1}}{1 + e^{\beta_0 + \beta_1 X_1}}$$



## Data
This data set has a binary response (outcome, dependent) variable called admit.

There are three predictor variables: gre, gpa and rank.

We will treat the variables gre and gpa as continuous.

The variable rank takes on the values 1 through 4. Institutions with a rank of 1 have the highest prestige, while those with a rank of 4 have the lowest.

## Data

```{r}
#reading the data from OARC stat website admission data:
mydata <- read.csv("https://stats.oarc.ucla.edu/stat/data/binary.csv")
#summary statistics of variables
summary(mydata)
#standard deviation of variables
sapply(mydata, sd)

```
##

$$Y = \begin{cases}
\text{ Yes admit} & \\
\text{ No admit} & \\
\end{cases} \;$$

. . .

$$\begin{split}
X_1 & = \text{ GRE} \\
\end{split}$$

## Understanding Odds and probability
$$\text{odds} = \frac{\pi}{1-\pi}$$

. . .

if prob of admit = $\frac{2}{3}$ then $\text{odds of admit } = \frac{2/3}{1-2/3} = 2$

. . .

if prob of admit = $\frac{1}{3}$ then $\text{odds of admit } = \frac{1/3}{1-1/3} = \frac{1}{2}$

. . .

if prob of admit = $\frac{1}{2}$ then  $\text{odds of admit } = \frac{1/2}{1-1/2} = 1$



## Example1: GLM using glm()

```{r}
# If we run lm
lm.admit <- lm(admit ~ gre, data = mydata)
summary(lm.admit) 
opar <- par(mfrow = c(2, 2), 
            mar = c(4.1, 4.1, 2.1, 1.1))
plot(lm.admit)

```
##

```{r}
#to estimate coefficient of the model we use glm with  
#the option family = binomial(link = "logit")
glm.mod <- glm(admit ~ gre, family = binomial(link = "logit"), data = mydata)

summary(glm.mod)

```

##

```{r}
coef(summary(glm.mod))

confint(glm.mod)
exp(coef(summary(glm.mod))[,1])
```

## Interpretation

The coefficient of GRE (slope of the model) interpreted as: For one unit increase of GRE the log-odds of admit expected to increases by 0.003582 on average.

. . .

The intercept is the expected log odds of someone getting admitted to graduate school when their GRE score is zero.

. . .

When we exponentiate the intercept then we get the expected odds of someone with GRE equal to zero. Exp(−2.901344)=0.05494932
Transforming to probability, p=0.05494932/(1+0.05494932)=0.052, we get the expected probability of someone with GRE equal to 0.

##

The exponentiation of the slope is the odds ratio of admission for a unit increase of GRE score which is: exp(0.003582)=1.003588
we can say the odds of getting admitted on average increase by 0.3%
 for one unit improvement of GRE score. 

. . .

What if someone improve their GRE by 50 unit?

. . .

The exponentiation function is increases multiplicative and not additive. Thus, for 50 more GRE score the odds of admission increases by a factor of 1.196115. In another word their odds of getting admitted increases by about 20%


##


```{r}
glm.mod.2 <- glm(admit ~ gre + gpa, family = binomial(link = "logit"), data = mydata)

summary(glm.mod.2)
coef(summary(glm.mod.2))
exp(coef(summary(glm.mod.2))[,1])
confint(glm.mod.2)
```
##
the odds of getting admitted on average increases by a factor of 2.1269
 for one unit improvement of GPA given GRE score is constant.

## Prediction 

```{r}
glm.mod.3 <- glm(admit ~ gre * gpa, family = binomial(link = "logit"), data = mydata)

summary(glm.mod.3)
coef(summary(glm.mod.3))
exp(coef(summary(glm.mod.3))[,1])
confint(glm.mod.3)
exp(3.367918022 + 500 * -0.004326605)
```

## Prediction and Classification
The interaction therm is an multiplication factor, for example for GPA, increases/decreases (here decreases) the odds of outcome by a factor of 0.9956827 for every unit of GRE.

. . .

For example if someone has GRE 500, for every 1 unit increase of their GPA then their odds of admission changes by a factor of 3.3354

. . .

Or we can first calculate 3.367918022+500*(−0.004326605)
 and then we can take the exponentiation.

## Regression of count data
Count data are observations that have only non-negative integer values.

. . .

Count can range from zero to some grater undetermined value.

. . .

Theoretically count can range from zero to infinity but in practice they are always limited to some lesser value.

## Some types of count data

A count of items or events occuring within a period of time.

. . .

Count of item or events occouring in a given geographical or spatial area.

. . .

Count of number of people having a particular disease, adjusted by the size of the population.

## Examples of a Poisson random variable

$$p(y|\mu ) = \frac{e^\mu\mu^y}{y!}$$

$$p(y=0|\mu=2 ) = \frac{e^2 2^0}{0!}=0.1353$$
$$p(y=1|\mu=2 ) = \frac{e^2  2^1}{1!}=0.2707$$

## Examples of a Poisson random variable


$p(y=2|\mu=2 ) = \frac{{e^2}{2}^2}{2!}=0.2707$


$p(y=3|\mu=2 ) = 0.1804$

$p(y=4|\mu=2 ) = 0.0902$

$p(y=5|\mu=2 ) = 0.0361$

$p(y=6|\mu=2 ) = 0.0120$

$p(y=7|\mu=2 ) = 0.0034$

$p(y=8|\mu=2 ) = 0.0009$

## Formalization of count model

link function: $g(\mu_i)=ln(\mu_i)$

. . .

  $g(\mu_i)= \beta_0 + \beta_1 X_{i1}$
  
. . .

$ln(\mu_i)= ln(E(y_i|x_i ; \beta_0 , \beta_1)) = \beta_0 + \beta_1 X_{i}$

. . . 


$$f(y_i|x_i ; \beta_0 , \beta_1) = \frac{{{e^{^{\beta_0 + \beta_1 x_i}}{(e^{\beta_0 + \beta_1 x_i}})^{y_i}}}}{y_i!}$$





## Example2: Count data - using glm()
```{r}

# library(COUNT)
data("rwm1984")
head(rwm1984)
summary(rwm1984)
?rwm1984

#centering the age predictor
rwm1984$cage <- rwm1984$age - mean(rwm1984$age)

#running glm
pois_m <- glm(docvis ~ outwork + cage, family = poisson(link = "log"), data = rwm1984)
#extract the outcome by summary
summary(pois_m)


```



##





